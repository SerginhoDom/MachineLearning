{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR45tYEBF/lXzuNh32meOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SerginhoDom/MachineLearning/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Отчет о проекте**"
      ],
      "metadata": {
        "id": "-JAg3hY2-fIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Введение**"
      ],
      "metadata": {
        "id": "d0mZ8dz3--Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом отчете рассматривается проект, направленный на создание системы автоматической коррекции ошибок в тексте с использованием нейронных сетей и методов обработки естественного языка (Natural Language Processing, NLP). Проект включает в себя обучение модели на размеченных данных, а также визуализацию статистической информации по языку."
      ],
      "metadata": {
        "id": "TSFO9EhW_Axm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Описание функциональности**"
      ],
      "metadata": {
        "id": "Kn-jknzb_A6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проект состоит из нескольких модулей:\n",
        "\n",
        "Main: Основной модуль, содержащий основной исполняемый код и логику\n",
        "работы программы.\n",
        "Visualisations: Модуль, ответственный за визуализацию статистической информации.\n",
        "Error Correction: Модуль, в котором реализованы функции для обнаружения и исправления ошибок в тексте.\n",
        "Text Processing: Модуль для предварительной обработки текстовых данных перед обучением модели.\n",
        "\n"
      ],
      "metadata": {
        "id": "weZbrkCW_A91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Используемые технологии**"
      ],
      "metadata": {
        "id": "z56GGJRL_BBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: Библиотека для создания и обучения нейронных сетей.\n",
        "\n",
        "Transformers (Hugging Face): Интерфейс для работы с предобученными моделями NLP, такими как BERT.\n",
        "\n",
        "NLTK: Библиотека для обработки текста естественного языка.\n",
        "\n",
        "Matplotlib и Seaborn: Библиотеки для визуализации данных."
      ],
      "metadata": {
        "id": "3BKJtuXU_BJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Реализация**"
      ],
      "metadata": {
        "id": "OLFCBI0t_BNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Токенизация и маскирование текста***"
      ],
      "metadata": {
        "id": "0TVsneWR_BQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_mask(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    masked_tokens = tokens.copy()\n",
        "    for i, token in enumerate(tokens):\n",
        "        masked_tokens[i] = '[MASK]'\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    return tokens_tensor"
      ],
      "metadata": {
        "id": "AHBJN0fCADS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пояснение: Этот блок кода реализует функцию для токенизации текста с помощью BERT-токенизатора и маскирования одного из токенов в каждом предложении. Маскирование токена позволяет модели предсказывать его значение."
      ],
      "metadata": {
        "id": "pKnGflU-AE4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Обучение модели***"
      ],
      "metadata": {
        "id": "V-7bq1F_AGx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dataset, model, optimizer, loss_fn, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        for text in train_dataset:\n",
        "            correct_text = preprocess_text(text)\n",
        "            corrected_text = correct_errors(correct_text, word_freq)\n",
        "            correct_tokens = tokenize_and_mask(correct_text)\n",
        "            incorrect_tokens = tokenize_and_mask(corrected_text)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=correct_tokens, labels=correct_tokens)\n",
        "            loss = loss_fn(outputs.logits.view(-1, tokenizer.vocab_size), correct_tokens.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "Epc7lUplAI4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пояснение: Этот блок кода отвечает за обучение модели на размеченных данных. Модель обучается на парах корректного и некорректного текста, минимизируя потери между предсказанными и истинными значениями."
      ],
      "metadata": {
        "id": "LbszMtWiAK3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Коррекция ошибок в тексте***"
      ],
      "metadata": {
        "id": "kMPt_-RqAZjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_text(input_text):\n",
        "    tokens_tensor = tokenize_and_mask(input_text)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=tokens_tensor)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    corrected_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(predictions[0]))\n",
        "    return corrected_text"
      ],
      "metadata": {
        "id": "-oVeWbUAAbh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пояснение: Этот блок кода реализует функцию для коррекции ошибок в тексте. Модель BERT используется для предсказания наиболее вероятных токенов, которые затем преобразуются в текст."
      ],
      "metadata": {
        "id": "rVomXRiHAdOF"
      }
    }
  ]
}